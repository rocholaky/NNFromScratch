{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Dataset Iris: Por Roberto Cholaky "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La parte siguiente del código irá dedicada a la obtención de los datos de entrenamiento y testeo. Se generará un arreglo de numpy que tenga todas las caracaterísticas de cada ejemplo en las filas. De este modo, si el arreglo es de M ejemplos y cada ejemplo tiene N características se tendrá que la matriz generada será de (M,N). Además para guardar los elementos de los labels en otro arregle de tamaño (M,C) donde C es la cantidad clases de clasificación (Esto ya que se utilizará one-hot-encoding), así cada fila en la matriz de características tendrá su label correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature data shape (150, 4)\n",
      "labels of data shape (150, 3)\n"
     ]
    }
   ],
   "source": [
    "#acá se define el nombre del archivo que se procede a trabajar:\n",
    "file_name = 'iris.data'\n",
    "#generamos listas para guardar los elementos del archivo de datos\n",
    "labels = list()\n",
    "data = list()\n",
    "one_hot_classes = dict()\n",
    "#Una vez llenas estas listas se procederá a pasar esta a un arreglo de numpy como se explicó en el apartado anterior:\n",
    "# abrimos el archivo file_name:\n",
    "with open(file_name) as file: \n",
    "    #procedemos a acceder a todas las filas del archivo:\n",
    "    for line in file: \n",
    "        #dividimos el archivo por comas donde sabemos que el último elemento es el label\n",
    "        splited_line = line.split(',')\n",
    "        #Pasamos a checkear si el arreglo queda vacio o no:\n",
    "        if splited_line != ['\\n']:\n",
    "            #Unimos las listas con los nuevos elementos\n",
    "            # el elemento [-1] indica que se accede al último elemento del arreglo\n",
    "            labels.append(splited_line[-1])\n",
    "            #acá accedemos a todos los elementos salvo el último.\n",
    "            data.append(splited_line[:-1])\n",
    "            \n",
    "#set de valores (valores sin repetir de la lista)\n",
    "classification_set = set(labels)\n",
    "n_C = len(classification_set)\n",
    "\n",
    "# se genera un loop donde enumerate entrega los elementos contados {0,1,2} \n",
    "# y para cada elemento en el set de labels (que son solamente 3 valores) se genera un one-hot encoding\n",
    "for index, value in enumerate(classification_set):\n",
    "    zero = np.zeros((1, n_C))\n",
    "    if index == 0:\n",
    "        zero[:,index] = 1\n",
    "        one_hot_classes[value]= zero \n",
    "    else:\n",
    "        zero[:,index] = 1\n",
    "        one_hot_classes[value]= zero\n",
    "\n",
    "#Procedemos a calcular el one-hot encoding de todos los labels.:\n",
    "for i, example in enumerate(labels):\n",
    "    labels[i] = one_hot_classes[example]\n",
    "\n",
    "#vstack es un método de la librería de numpy la que toma dos listas y las apila como filas. De este modo,\n",
    "# si tenemos N filas de M elementos este nos entrega una función de (N,M).\n",
    "data = np.vstack(data).astype('float64')\n",
    "labels = np.vstack(labels)\n",
    "#como el dataset viene ordenado es necesario desordenarlo para que los ejemplos no se encuentren todos juntos.\n",
    "# de este modo, permutation de numpy permite entregar un arreglo de indices desordenados que permite que el arreglo quede\n",
    "#desordenado.\n",
    "permutation = np.random.permutation(data.shape[0])\n",
    "data = data[permutation,:]\n",
    "labels = labels[permutation]\n",
    "#print the shape of data and labels:\n",
    "print('feature data shape '+ str(data.shape))\n",
    "print('labels of data shape ' + str(labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalización de los datos: \n",
    "Es sabido que una manera de obtener un entrenamiento más rápido es mediante el uso de normalización de los datos de entrada, esto se debe a que datos normalizados permite que la región de optimización que se tiene sea más uniforme y por ende el método de descenso de gradiente sea más rápido debido a que los gradientes permiten un avance más directo hacia los mínimos. En los dos siguientes bloques de código se utilizan dos métodos conocidos de normalización, uno es el visto en clases y otro es uno enseñado en los cursos de coursera de deeplearning.ia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método Número 1: \n",
    "normalización de datos mediante el uso de la media $\\mu$ y la desviación estandard $\\sigma$:\n",
    "Este es la fórmula típica de normalización de datos en estadística que permite pasar los datos desde su distribución a una normal (0, 1) como los datos tienen en cada coordenada una distribución propia, se calcula la media  y desviación estandard entre los ejemplos de todas las coordenadas para después restarlos y usar para cada una de ellas la formula: \n",
    "\n",
    "$$Z_{i} = \\frac{x_{i}-\\mu}{\\sigma^{2}}$$\n",
    "\n",
    "Esto permite que los datos se agrupen cerca de $[-1, 1]$ con una probabilidad de 0.66. La gran diferencia con el otro método es que no asegura que el valor esté entre 0 y 1 sino que la mayoría de estos lo estarán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape de la media (1, 4)\n",
      "shape de la desviación estandard (1, 4)\n"
     ]
    }
   ],
   "source": [
    "mu_data = np.mean(data, axis= 0, keepdims = True)\n",
    "std_data = np.std(data, axis = 0, keepdims = True)\n",
    "print('shape de la media ' + str(mu_data.shape))\n",
    "print('shape de la desviación estandard '+str(std_data.shape))\n",
    "#data Normalization: \n",
    "data = (data-mu_data)/np.square(std_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método Número 2: \n",
    "normalización de datos mediante el uso de máximos y mínimos:\n",
    "Esta fue la forma de normalizar los datos vista en clases, que permite que los valores se encuentren entre el 0 y el 1.\n",
    "Esto ya que divide la diferencia entre el dato actual y el valor más pequeño del arreglo (por cada coordenada) y luego este valor lo divide por la diferencia entre el máximo valor de dicha coordenada y el menor que sería la diferencia máxima que puede existir en el arreglo.\n",
    "\n",
    "$$Z_{i} = \\frac{x_{i}-min}{max - min}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_max = np.max(data, axis=0)\n",
    "data_min = np.min(data, axis=0)\n",
    "data = (data-data_min)/(data_max-data_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Cross validation:\n",
    "Es un métod de resampling el cual permite separar los datos en grupos donde uno de estos será utilizado para testear el entrenamiento de los datos. La idea se encuentra en que la composición del set de entrenamiento se puede variar con los distintos grupos y por ende se puede probar con distintos subgrupos de los datos.  Para esta tarea se dividió en 2 grupos donde uno contiene el 80% de los datos y el otro el 20% que son training y testing respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(30, 4)\n"
     ]
    }
   ],
   "source": [
    "#cross validation_set: generates training and test set with a 80% and 20% respectively from the dataset:\n",
    "def cross_validation_set(X, Y): \n",
    "    #obtenemos el número de ejemplos\n",
    "    M= X.shape[0]\n",
    "    #permutamos los datos para que los grupos no sean siempre iguales si X e Y son los mismos\n",
    "    permutation = np.random.permutation(M)\n",
    "    #barajamos los sets de datos\n",
    "    X_shuffled = X[permutation]\n",
    "    Y_shuffled = Y[permutation]\n",
    "    \n",
    "    #sacamos el 80% para el training set y el 20% para el test set.\n",
    "    # recordar que en numpy :fila son todos los elementos hast esa fila (sin ella incluida) \n",
    "    #y fila: es desde esa fila todos los elementos restantes\n",
    " \n",
    "    train_limit = int(M*0.8)\n",
    "    X_train = X_shuffled[:train_limit]\n",
    "    Y_train = Y_shuffled[:train_limit]\n",
    "    X_test = X_shuffled[train_limit:]\n",
    "    Y_testing = Y_shuffled[train_limit:]\n",
    "    return X_train, Y_train, X_test, Y_testing\n",
    "\n",
    "X_train, Y_train, X_test, Y_testing = cross_validation_set(data, labels)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generación de funciones de activación : \n",
    "#función tanh\n",
    "def tanh(X):\n",
    "    #numpy exp permite aplicar la exponencial a todos los elementos de un arreglo. Se aplica punto a punto.\n",
    "    exp = np.exp(X)\n",
    "    exp_ = np.exp(-1*X)\n",
    "    #la resta, suma y división de este tipo en numpy son elemento a elemento.\n",
    "    tanh = (exp-exp_)/(exp+exp_)\n",
    "    return tanh\n",
    "\n",
    "#función de salida softmax: \n",
    "def softmax(X):\n",
    "    exp = np.exp(X, dtype= np.float64)\n",
    "    # se suman todos los elementos del vector por cada fila queda un vector de (ejemplos,)\n",
    "    sum_exp = np.sum(exp, axis=1)\n",
    "    #como queremos que sea de (ejemplos, 1) usamos la función expand_dims para obtener la dimensión faltante.\n",
    "    sum_exp = np.expand_dims(sum_exp, -1)\n",
    "    #este término se agrega ya que así la función softmax no sufrirá de indefinición (divisiones por cero)\n",
    "    return exp/(sum_exp+0.002)\n",
    "\n",
    "#función relu:\n",
    "def relu(x):\n",
    "    #la función np.where entrega una matriz donde si se cumple la condición x>0 el valor será el mismo x, sin x<0 entonces el\n",
    "    #valor en la matriz será cero.\n",
    "    return np.where(x>0, x, 0)\n",
    "\n",
    "#funciones derivada de las funciones de activación:\n",
    "#derivada de tanh:\n",
    "def dtanh(x):\n",
    "    return 1 - np.square(tanh(x))\n",
    "\n",
    "#derivada de la función relu:\n",
    "def drelu(x):\n",
    "    #la función np.where entrega una matriz donde si se cumple la condición x>0 el valor será el mismo x, sin x<0 entonces el\n",
    "    #valor en la matriz será cero.\n",
    "    return np.where(x>=0, 1, 0)\n",
    "\n",
    "\n",
    "# comienzo de la clase MLP: \n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, X, Y, n_layers, n_hidden_units, named_labels, activation='tanh'):\n",
    "        #se genera el diccionario de elementos que utilizará la red neuronal:\n",
    "        #pesos:\n",
    "        W = dict()\n",
    "        #bias\n",
    "        b = dict()\n",
    "        #derivadas de Z = WX+b\n",
    "        dZ = dict()\n",
    "        #derivadas de los pesos\n",
    "        dW = dict()\n",
    "        #derivada del bias\n",
    "        db = dict()\n",
    "        #los valores del forward A = f(Z)\n",
    "        A_cache = dict()\n",
    "        #los valores del forward de Z\n",
    "        Z_cache = dict()\n",
    "        \n",
    "        # se guardan elementos como la función activación\n",
    "        self.__activation__ = activation\n",
    "        # los labels de las clases a clasificar (esto se usará en la matriz de confusión)\n",
    "        self.named_labels = named_labels\n",
    "        #el número de capas\n",
    "        self.n_layers = n_layers\n",
    "        #el learning rate se setea a un valor estandard en caso de que el usuario no lo indique\n",
    "        self.learning_rate = 0.1\n",
    "        \n",
    "        #se empieza la generación de los pesos mediante la cantidad de elementos del vector X: \n",
    "        #queremos que si X es de (M, N) y la primera capa es de 10 elementos, W sea de (N, 10)\n",
    "        first_shape = X.shape[1]\n",
    "        #el número de hiddent units indicará la cantidad de neuronas de cada capa:\n",
    "        second_shape = n_hidden_units\n",
    "        for layer in range(n_layers):\n",
    "            # el usuario puede elegir entre dos funciones de activación tanh o relu por lo que cada una generará activaciones \n",
    "            # diferentes ya que se ha probado que afecta el rendimiento de las redes:\n",
    "            if activation == 'tanh':\n",
    "                # guardamos el elemento en el diccionario\n",
    "                #np.sqrt() es equivalente a la raiz cuadrada. \n",
    "                #np.random.rand entrega una matriz de forma (first_shape, second_shape) cuyos elemntos están entre 0 y 1\n",
    "                W['W'+str(layer+1)] = (1/np.sqrt(second_shape))*np.random.rand(first_shape, second_shape)\n",
    "            elif activation == 'relu':\n",
    "                W['W'+str(layer+1)] = (1/np.sqrt(second_shape))*np.random.rand(first_shape, second_shape)\n",
    "            else:\n",
    "                raise Exception('Only tanh or relu activation functions work. Use those instead')\n",
    "            #np.zeros genera un vector de ceros que tiene forma: (1,second_shape)\n",
    "            b['b'+str(layer+1)] = np.zeros((1,second_shape))\n",
    "            #se actualiza las dimensiones de la red.\n",
    "            first_shape = second_shape\n",
    "        #inicializamos la capa de salida: \n",
    "        W['W'+str(self.n_layers+1)] = (2/np.sqrt(second_shape))*np.random.rand(n_hidden_units, Y.shape[1])\n",
    "        b['b'+str(self.n_layers+1)] = np.zeros((1, Y.shape[1]))\n",
    "        #se genera el diccionario de parametros.\n",
    "        self.parameters = dict()\n",
    "        self.parameters['W'] = W\n",
    "        self.parameters['b'] = b\n",
    "        self.parameters['dZ'] = dZ\n",
    "        self.parameters['dW'] = dW\n",
    "        self.parameters['db'] = db\n",
    "        self.parameters['A_cache'] = A_cache\n",
    "        self.parameters['Z_cache'] = Z_cache\n",
    "    \n",
    "    def forwardPass(self, X, parameters):\n",
    "        W = parameters['W']\n",
    "        b = parameters['b']\n",
    "        A_cache = dict()\n",
    "        Z_cache = dict()\n",
    "        In = X \n",
    "        #se guarda el input en el diccionario A: esto se usará en BP\n",
    "        A_cache['A0']= X\n",
    "        #se recorren las capas:\n",
    "        for layer in range(self.n_layers):\n",
    "            #se genera Z: Wx+b, donde np.dot calcula la multiplicación matricial \n",
    "            Z = np.dot(In, W['W'+str(layer+1)])+b['b'+str(layer+1)]\n",
    "            #guardamos el Z en el diccionario, esto servirá para BP:\n",
    "            Z_cache['Z'+str(layer+1)] = Z \n",
    "            #dependiendo de la activación que escogío el usuario se utiliza esa función de activación:\n",
    "            #como ya en init revisabamos que self.activation fuese tanh o relu no es necesario hacerlo de nuevo: (raise exception)\n",
    "            if self.__activation__ == 'tanh':\n",
    "                A = tanh(Z)\n",
    "            else:\n",
    "                A = relu(Z)\n",
    "            #se guarda A en el diccionario (util para BP)\n",
    "            A_cache['A'+str(layer+1)] = A\n",
    "            In = A\n",
    "        # se calcula la capa de salida:\n",
    "        Z = np.dot(In, W['W'+str(self.n_layers+1)])+b['b'+str(self.n_layers+1)]\n",
    "        Z_cache['Z'+str(self.n_layers+1)] = Z \n",
    "        # se calcula la predicción de salida dado un input\n",
    "        Y_pred = softmax(Z) \n",
    "        \n",
    "        # se genera otro diccionario para no afectar el anterior y así poder agregar en un futuro el set de validación de ser \n",
    "        #necesario:\n",
    "        model_parameters = dict()\n",
    "        model_parameters['W'] = W\n",
    "        model_parameters['b'] = b\n",
    "        model_parameters['dW'] = parameters['dW']\n",
    "        model_parameters['db'] = parameters['db']\n",
    "        model_parameters['dZ'] = parameters['dZ']\n",
    "        model_parameters['A_cache'] = A_cache\n",
    "        model_parameters['Z_cache'] = Z_cache\n",
    "        \n",
    "        # el usuario puede elegir entre usar el diccionario de parámetros o no.\n",
    "        return Y_pred, model_parameters \n",
    "    \n",
    "    #para el loss se utilizó entropía cruzada:\n",
    "    def Loss(self, Y_pred, Y_real):\n",
    "        #se aplica el logaritmo de cada elemento de la predicción:\n",
    "        #el término agregado es para que no hayan problemas con indefiniciones del logaritmo.\n",
    "        log_Ypred = np.log(Y_pred+0.000002)\n",
    "        #se hace la multiplicación de cada elemento por su label real:\n",
    "        # el operador * en numpy es el operador multiplicación elemento a elemento\n",
    "        Ylog_Ypred = -Y_real*log_Ypred\n",
    "        #se calcula la suma de todos estos logaritmos por ejemplo por lo que queda el vector con forma (ejemplos,)\n",
    "        L_i = np.sum(Ylog_Ypred, axis= 1)\n",
    "        #se calcula el promedio de todos los ejemplos para obtener un único loss.\n",
    "        loss = np.mean(L_i)\n",
    "        return loss\n",
    "        \n",
    "    #funcion que dada una prediccion y un set de parametros permite realizar BP:    \n",
    "    def backPropagation(self, Y_pred, Y, parameters):\n",
    "        W = parameters['W']  \n",
    "        A_cache = parameters['A_cache']\n",
    "        Z_cache = parameters['Z_cache']\n",
    "        dZ = parameters['dZ']\n",
    "        dW = parameters['dW'] \n",
    "        db = parameters['db']\n",
    "        #se guarda el número de ejemplos:\n",
    "        M = Y.shape[0]\n",
    "        #Dz es de forma (M, C) donde M es el numero de ejemplos y C el número de clases:\n",
    "        dZ['Z'+str(self.n_layers+1)] = (Y_pred - Y)\n",
    "        # DW para la última capa tiene forma (n_[l-1], C) donde n_[l-1] es la cantidad de unidades ocultas de la capa l-1 \n",
    "        #y C el numero de clases: \n",
    "        #multiplicamos A.T (el traspuesto de A) por dZ para obtener una forma (n_[l-1], M)X(M, C)= (n_[l-1], C).\n",
    "        dW['W'+str(self.n_layers+1)] = np.dot(A_cache['A'+str(self.n_layers)].T, dZ['Z'+str(self.n_layers+1)])\n",
    "        #el parametro keepdims permite que al salida sea de forma (n, 1) donde si no se usa sería de forma (n,)\n",
    "        db['b'+str(self.n_layers+1)] = (1/M)*np.sum(dZ['Z'+str(self.n_layers+1)], axis=0, keepdims=True)\n",
    "        #corremos el BP por todas las capas:\n",
    "        for layer in range(self.n_layers, 0, -1):\n",
    "            #como dZ[l] es de forma (M, n_[l]) y W[l] es de forma (n_[l-1], n_l[l]) dZ[l]xW.T devuelve las dimensiones correctas\n",
    "            #porque dZ[l-1] es de la forma (M, n[l-1])\n",
    "            if self.__activation__ == 'tanh':\n",
    "                dZ['Z'+str(layer)] = np.dot(dZ['Z'+str(layer+1)], W['W'+str(layer+1)].T)*dtanh(Z_cache['Z'+str(layer)])\n",
    "            else:\n",
    "                dZ['Z'+str(layer)] = np.dot(dZ['Z'+str(layer+1)], W['W'+str(layer+1)].T)*drelu(Z_cache['Z'+str(layer)])\n",
    "            # DW para la capa l-esima tiene forma (n_[l-1], n_[l]) entonces multiplicamos A.T por dZ \n",
    "            #en cuyo caso tenemos (n_[l-1], M)X(M, n_[l])= (n_[l-1], C): \n",
    "            dW['W'+str(layer)] = (1/M)*np.dot(A_cache['A'+str(layer-1)].T, dZ['Z'+str(layer)])\n",
    "            db['b'+str(layer)] = (1/M)*np.sum(dZ['Z' + str(layer)], axis=0, keepdims=True)\n",
    "        #guardamos los parametros:    \n",
    "        parameters['dZ'] = dZ\n",
    "        parameters['dW'] = dW\n",
    "        parameters['db'] = db\n",
    "        #actualizamos los pesos antes de devolverlos\n",
    "        parameters = self.update_parameters(parameters)\n",
    "        return parameters\n",
    "        \n",
    "    \n",
    "    # función que permite actualiza pesos dado un diccionario de derivadas:\n",
    "    def update_parameters(self, parameters):\n",
    "        W = parameters['W']\n",
    "        b = parameters['b']\n",
    "        dW = parameters['dW'] \n",
    "        db = parameters['db']\n",
    "        for layer in range(1, self.n_layers+1, 1):\n",
    "            #aplicamos gradiente descendiente:\n",
    "            W['W'+str(layer+1)] = W['W'+str(layer+1)]-self.learning_rate*dW['W'+str(layer+1)]\n",
    "            b['b'+str(layer+1)] = b['b'+str(layer+1)]-self.learning_rate*db['b'+str(layer+1)]\n",
    "        parameters['W'] = W\n",
    "        parameters['b'] = b\n",
    "        self.parameters\n",
    "        return parameters\n",
    "        \n",
    "    \n",
    "    #funcion que permite el cálculo de accuracy: \n",
    "    def calc_accuracy(self, Y_pred, Y):\n",
    "        #expandimos la última dimensión para que tengan forma (n, 1) para cualquier n.\n",
    "        #recordemos que el término -1 hace referencia al último elemento.\n",
    "        pred = np.expand_dims(np.argmax(Y_pred, axis=-1), -1)\n",
    "        real = np.expand_dims(np.argmax(Y, axis=-1),-1)\n",
    "        #el operador == en numpy hace que el vector resultante sea de (true o false), donde si sumamos todos los true ( o 1)\n",
    "        # obtenemos la cantidad de veces que se calculó correctamente el resultado:\n",
    "        acc = np.sum((pred==real))/Y.shape[0]\n",
    "        return acc\n",
    "    \n",
    "    #función predecir: predice dado un input y sus parámetros la clase del input\n",
    "    def predict(self, X, parameters):\n",
    "        W = parameters['W']\n",
    "        b = parameters['b']\n",
    "        In = X \n",
    "        for layer in range(self.n_layers):\n",
    "            Z = np.dot(In, W['W'+str(layer+1)])+b['b'+str(layer+1)]\n",
    "            if self.__activation__ == 'tanh':\n",
    "                A = tanh(Z)\n",
    "            elif self.__activation__ == 'relu':\n",
    "                A = relu(Z)\n",
    "            In = A\n",
    "        Z = np.dot(In, W['W'+str(self.n_layers+1)])+b['b'+str(self.n_layers+1)]\n",
    "        Y_pred = softmax(Z) \n",
    "        return Y_pred\n",
    "    #función que calcula la matriz de confusión:        \n",
    "    def confusion_matrix(self, Y_pred, Y):\n",
    "        # se utiliza np.argmax para obtener el indice de mayor valor en la salida de la función softmax\n",
    "        # np.expand dims expande la última dimensión para obtener un vector de (n,1)\n",
    "        real_labels = np.expand_dims(np.argmax(Y, axis=-1),-1)\n",
    "        pred_labels = np.expand_dims(np.argmax(Y_pred, axis=-1), -1)\n",
    "        #con los nombres de los labels crearemos la matriz de confuisón\n",
    "        N_labels = len(self.named_labels)\n",
    "        #generamos una matriz llena de ceros que iremos llenando\n",
    "        conf_M = np.zeros((N_labels, N_labels))\n",
    "        for example in range(real_labels.shape[0]):\n",
    "            #recorremos todos los ejemplos\n",
    "            #obtenemos los indices calculados anteriormente\n",
    "            row = real_labels[example]\n",
    "            column = pred_labels[example]\n",
    "            #le sumamos uno a la posición. Recordemos que si row == column entonces la predicción es correcta\n",
    "            conf_M[row, column] += 1\n",
    "        #utilizamos un frame de pandas para poner filas y columnas con nombre de las clases de clasificación.\n",
    "        conf_M = pd.DataFrame(conf_M, self.named_labels, self.named_labels)\n",
    "        return conf_M\n",
    "        \n",
    "    #acá se genera el entrenamiento, la función genera los training y test set sola mediante cross-validation:\n",
    "    def train(self, X, Y, Epochs, learning_rate):\n",
    "        #obtenemos los parámetros\n",
    "        parameters = self.parameters\n",
    "        #generamos listas para los loss y accuracy:\n",
    "        training_loss = list()\n",
    "        training_accuracy = list()\n",
    "        \n",
    "        #se define el learning rate:\n",
    "        self.learning_rate= learning_rate\n",
    "        #se separa la data\n",
    "        X_train, Y_train, X_test, Y_test = cross_validation_set(X, Y)\n",
    "        \n",
    "        #iteraremos el entrenamiento por le número de epocas:\n",
    "        for epoch in range(1, Epochs+1):\n",
    "            #se genera la predicción y se obtienen los parametros (Z_cache, A_cache)\n",
    "            Y_pred, parameters = self.forwardPass(X_train, parameters)\n",
    "            \n",
    "            #se calcula el error:\n",
    "            train_loss = self.Loss(Y_pred, Y_train)\n",
    "            \n",
    "            #se calcula el accuracy:\n",
    "            train_acc = self.calc_accuracy(Y_pred, Y_train)\n",
    "            \n",
    "            #se agregan a las listas:\n",
    "            training_loss.append(train_loss)\n",
    "            training_accuracy.append(train_acc)\n",
    "            \n",
    "            #cada 5 epocas se printeará el valor loss y accuracy\n",
    "            if epoch%5==0:\n",
    "                print('Epoch {N_epoch} is train loss: {loss}, train accuracy: {accuracy}'.format(N_epoch=epoch, loss=train_loss,\\\n",
    "                                                                                                            accuracy= train_acc))\n",
    "            #se actualizan los pesos    \n",
    "            parameters = self.backPropagation(Y_pred, Y_train, parameters)\n",
    "            \n",
    "        \n",
    "        #se predice una salida:\n",
    "        Y_pred_test = self.predict(X_test, parameters)\n",
    "        #se calcula su loss:\n",
    "        testing_loss = self.Loss(Y_pred_test, Y_test)\n",
    "        \n",
    "        #y su accuracy:\n",
    "        testing_accuracy = self.calc_accuracy(Y_pred_test, Y_test) \n",
    "        \n",
    "        #se printean los resultados de entrenamiento como el loss medio y el accuracy final de entrenamiento\n",
    "        #junto con el loss de testeo y el accuracy de testeo\n",
    "        print('Trainig is done:')\n",
    "        print('the training loss is {loss} and the training accuracy is {acc}'.format(loss = np.mean(training_loss), acc=train_acc))\n",
    "        print('the testing loss is {loss} and the testing accuracy is {acc}'.format(loss = testing_loss, acc= testing_accuracy))\n",
    "        \n",
    "        #se grafica el costo por epoch\n",
    "        fig, ax = plt.subplots()\n",
    "        train = ax.plot(range(1, Epochs+1), training_loss, '-b', label='training loss')\n",
    "        ax.set_title('models loss with {epochs} epochs of training'.format(epochs=Epochs))\n",
    "        ax.set_xlabel('Epochs')\n",
    "        ax.set_ylabel('loss')\n",
    "        ax.set_ylim([0,5])\n",
    "        plt.show()\n",
    "        \n",
    "        # se grafica el accuracy por epoch\n",
    "        fig, ax = plt.subplots()\n",
    "        train = ax.plot(range(1, Epochs+1), training_accuracy, '-b', label='training accuracy')\n",
    "        ax.set_title('models accuracy with {epochs} epochs of training'.format(epochs=Epochs))\n",
    "        ax.set_xlabel('Epochs')\n",
    "        ax.set_ylabel('accuracy')\n",
    "        ax.set_ylim([0,1])\n",
    "        plt.show()\n",
    "        plt.show()\n",
    "        print('plot of the confusion matrix, where the rows represent the real data and the columns the predicted data:')\n",
    "        \n",
    "        #se devuelve la matriz de confusión:\n",
    "        cM = self.confusion_matrix(Y_test, Y_pred_test)\n",
    "        print(cM)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se prueba la primera estructura: una red de 4 capas ocultas, 10 unidades por capa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_classes = list(classification_set)\n",
    "myMlp = MLP(data, labels, n_layers = 4, n_hidden_units=10, named_labels=named_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se entrena por 700 epochs con un learning rate de 0.02:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 is train loss: 1.273300675240498, train accuracy: 0.5083333333333333\n",
      "Epoch 10 is train loss: 1.3602136887246261, train accuracy: 0.55\n",
      "Epoch 15 is train loss: 1.2822150854889178, train accuracy: 0.49166666666666664\n",
      "Epoch 20 is train loss: 1.2439395301540865, train accuracy: 0.5416666666666666\n",
      "Epoch 25 is train loss: 1.1751058516923718, train accuracy: 0.5\n",
      "Epoch 30 is train loss: 1.1418087840507676, train accuracy: 0.5416666666666666\n",
      "Epoch 35 is train loss: 1.0928244384697383, train accuracy: 0.5\n",
      "Epoch 40 is train loss: 1.0641332138904518, train accuracy: 0.5416666666666666\n",
      "Epoch 45 is train loss: 1.0302509276628602, train accuracy: 0.5\n",
      "Epoch 50 is train loss: 1.007971851562837, train accuracy: 0.5416666666666666\n",
      "Epoch 55 is train loss: 0.9846677271897697, train accuracy: 0.49166666666666664\n",
      "Epoch 60 is train loss: 0.9684317721667043, train accuracy: 0.5416666666666666\n",
      "Epoch 65 is train loss: 0.9523089832205326, train accuracy: 0.49166666666666664\n",
      "Epoch 70 is train loss: 0.9407549950239741, train accuracy: 0.5416666666666666\n",
      "Epoch 75 is train loss: 0.9294165415585508, train accuracy: 0.49166666666666664\n",
      "Epoch 80 is train loss: 0.9211339304618006, train accuracy: 0.5416666666666666\n",
      "Epoch 85 is train loss: 0.9129281055349381, train accuracy: 0.49166666666666664\n",
      "Epoch 90 is train loss: 0.9068120741346064, train accuracy: 0.5416666666666666\n",
      "Epoch 95 is train loss: 0.9006313094720312, train accuracy: 0.49166666666666664\n",
      "Epoch 100 is train loss: 0.8959122440492567, train accuracy: 0.5416666666666666\n",
      "Epoch 105 is train loss: 0.8910327424069154, train accuracy: 0.5\n",
      "Epoch 110 is train loss: 0.887202438543433, train accuracy: 0.5416666666666666\n",
      "Epoch 115 is train loss: 0.8831591069228911, train accuracy: 0.49166666666666664\n",
      "Epoch 120 is train loss: 0.879889037136012, train accuracy: 0.5416666666666666\n",
      "Epoch 125 is train loss: 0.8763854202049743, train accuracy: 0.5\n",
      "Epoch 130 is train loss: 0.873464311355837, train accuracy: 0.55\n",
      "Epoch 135 is train loss: 0.8703125592171013, train accuracy: 0.5\n",
      "Epoch 140 is train loss: 0.8676052273253435, train accuracy: 0.55\n",
      "Epoch 145 is train loss: 0.8646882377264885, train accuracy: 0.5083333333333333\n",
      "Epoch 150 is train loss: 0.8621105954611911, train accuracy: 0.5583333333333333\n",
      "Epoch 155 is train loss: 0.8593586687605714, train accuracy: 0.5083333333333333\n",
      "Epoch 160 is train loss: 0.8568630924321775, train accuracy: 0.5583333333333333\n",
      "Epoch 165 is train loss: 0.8542390366471314, train accuracy: 0.525\n",
      "Epoch 170 is train loss: 0.8518051695694041, train accuracy: 0.5666666666666667\n",
      "Epoch 175 is train loss: 0.8492934650072839, train accuracy: 0.525\n",
      "Epoch 180 is train loss: 0.8469203677202419, train accuracy: 0.5666666666666667\n",
      "Epoch 185 is train loss: 0.8445170757344953, train accuracy: 0.5666666666666667\n",
      "Epoch 190 is train loss: 0.8422131183274219, train accuracy: 0.575\n",
      "Epoch 195 is train loss: 0.8399139983367482, train accuracy: 0.5583333333333333\n",
      "Epoch 200 is train loss: 0.8376823473064451, train accuracy: 0.575\n",
      "Epoch 205 is train loss: 0.8354690923112873, train accuracy: 0.5666666666666667\n",
      "Epoch 210 is train loss: 0.8332918179129265, train accuracy: 0.575\n",
      "Epoch 215 is train loss: 0.8311222836431952, train accuracy: 0.5666666666666667\n",
      "Epoch 220 is train loss: 0.8289538670666232, train accuracy: 0.575\n",
      "Epoch 225 is train loss: 0.8267662638180446, train accuracy: 0.575\n",
      "Epoch 230 is train loss: 0.8245455263814176, train accuracy: 0.575\n",
      "Epoch 235 is train loss: 0.8222766188129351, train accuracy: 0.575\n",
      "Epoch 240 is train loss: 0.8199475494782142, train accuracy: 0.575\n",
      "Epoch 245 is train loss: 0.817548015604166, train accuracy: 0.575\n",
      "Epoch 250 is train loss: 0.8150692975084602, train accuracy: 0.575\n",
      "Epoch 255 is train loss: 0.8125035373430985, train accuracy: 0.575\n",
      "Epoch 260 is train loss: 0.8098431297024511, train accuracy: 0.575\n",
      "Epoch 265 is train loss: 0.8070802962039954, train accuracy: 0.575\n",
      "Epoch 270 is train loss: 0.8042067611753834, train accuracy: 0.575\n",
      "Epoch 275 is train loss: 0.8012135487939498, train accuracy: 0.575\n",
      "Epoch 280 is train loss: 0.7980908055023267, train accuracy: 0.5833333333333334\n",
      "Epoch 285 is train loss: 0.7948276353331111, train accuracy: 0.5833333333333334\n",
      "Epoch 290 is train loss: 0.7914119141697952, train accuracy: 0.5833333333333334\n",
      "Epoch 295 is train loss: 0.7878300750299096, train accuracy: 0.6\n",
      "Epoch 300 is train loss: 0.7840668507337022, train accuracy: 0.6083333333333333\n",
      "Epoch 305 is train loss: 0.7801049625688847, train accuracy: 0.625\n",
      "Epoch 310 is train loss: 0.7759247388799716, train accuracy: 0.625\n",
      "Epoch 315 is train loss: 0.771503642896035, train accuracy: 0.6333333333333333\n",
      "Epoch 320 is train loss: 0.7668156817023417, train accuracy: 0.6333333333333333\n",
      "Epoch 325 is train loss: 0.7618306581002234, train accuracy: 0.6333333333333333\n",
      "Epoch 330 is train loss: 0.7565132125767439, train accuracy: 0.6416666666666667\n",
      "Epoch 335 is train loss: 0.750821581805052, train accuracy: 0.6416666666666667\n",
      "Epoch 340 is train loss: 0.7447059699780596, train accuracy: 0.6416666666666667\n",
      "Epoch 345 is train loss: 0.7381063852658911, train accuracy: 0.6416666666666667\n",
      "Epoch 350 is train loss: 0.7309497287829899, train accuracy: 0.6583333333333333\n",
      "Epoch 355 is train loss: 0.7231458269502832, train accuracy: 0.6666666666666666\n",
      "Epoch 360 is train loss: 0.7145819537711589, train accuracy: 0.6666666666666666\n",
      "Epoch 365 is train loss: 0.7051151731358855, train accuracy: 0.6666666666666666\n",
      "Epoch 370 is train loss: 0.694561509027366, train accuracy: 0.6666666666666666\n",
      "Epoch 375 is train loss: 0.6826804834688304, train accuracy: 0.675\n",
      "Epoch 380 is train loss: 0.6691529269616637, train accuracy: 0.675\n",
      "Epoch 385 is train loss: 0.6535492603010294, train accuracy: 0.6916666666666667\n",
      "Epoch 390 is train loss: 0.6352852213326762, train accuracy: 0.7166666666666667\n",
      "Epoch 395 is train loss: 0.6135643371250374, train accuracy: 0.7416666666666667\n",
      "Epoch 400 is train loss: 0.5873180211454428, train accuracy: 0.775\n",
      "Epoch 405 is train loss: 0.5551940724472468, train accuracy: 0.7916666666666666\n",
      "Epoch 410 is train loss: 0.5157550901765912, train accuracy: 0.7916666666666666\n",
      "Epoch 415 is train loss: 0.46826174920194713, train accuracy: 0.8416666666666667\n",
      "Epoch 420 is train loss: 0.41443409643346624, train accuracy: 0.8583333333333333\n",
      "Epoch 425 is train loss: 0.36010520165246085, train accuracy: 0.8833333333333333\n",
      "Epoch 430 is train loss: 0.31290317334512535, train accuracy: 0.9\n",
      "Epoch 435 is train loss: 0.27632321338825133, train accuracy: 0.9166666666666666\n",
      "Epoch 440 is train loss: 0.24873332155814504, train accuracy: 0.9333333333333333\n",
      "Epoch 445 is train loss: 0.22725904019442963, train accuracy: 0.95\n",
      "Epoch 450 is train loss: 0.20988864791271927, train accuracy: 0.95\n",
      "Epoch 455 is train loss: 0.1954495500787585, train accuracy: 0.9583333333333334\n",
      "Epoch 460 is train loss: 0.1832288006603166, train accuracy: 0.9583333333333334\n",
      "Epoch 465 is train loss: 0.17274963964637877, train accuracy: 0.9583333333333334\n",
      "Epoch 470 is train loss: 0.16366931177529231, train accuracy: 0.9583333333333334\n",
      "Epoch 475 is train loss: 0.15573092154865797, train accuracy: 0.9583333333333334\n",
      "Epoch 480 is train loss: 0.1487372939100228, train accuracy: 0.9583333333333334\n",
      "Epoch 485 is train loss: 0.1425345946597946, train accuracy: 0.9583333333333334\n",
      "Epoch 490 is train loss: 0.1370011095537881, train accuracy: 0.9583333333333334\n",
      "Epoch 495 is train loss: 0.13203923846633772, train accuracy: 0.9583333333333334\n",
      "Epoch 500 is train loss: 0.12756969361784437, train accuracy: 0.9583333333333334\n",
      "Epoch 505 is train loss: 0.12352726591484009, train accuracy: 0.9583333333333334\n",
      "Epoch 510 is train loss: 0.1198577166010883, train accuracy: 0.9583333333333334\n",
      "Epoch 515 is train loss: 0.11651547597693765, train accuracy: 0.9583333333333334\n",
      "Epoch 520 is train loss: 0.11346192045739215, train accuracy: 0.9583333333333334\n",
      "Epoch 525 is train loss: 0.11066406506147887, train accuracy: 0.9583333333333334\n",
      "Epoch 530 is train loss: 0.10809355629224739, train accuracy: 0.9583333333333334\n",
      "Epoch 535 is train loss: 0.10572588452669104, train accuracy: 0.9583333333333334\n",
      "Epoch 540 is train loss: 0.10353975900911334, train accuracy: 0.9583333333333334\n",
      "Epoch 545 is train loss: 0.10151660517571982, train accuracy: 0.9583333333333334\n",
      "Epoch 550 is train loss: 0.0996401555169755, train accuracy: 0.9583333333333334\n",
      "Epoch 555 is train loss: 0.09789611310834433, train accuracy: 0.9583333333333334\n",
      "Epoch 560 is train loss: 0.09627187244119521, train accuracy: 0.9583333333333334\n",
      "Epoch 565 is train loss: 0.09475628604363627, train accuracy: 0.9583333333333334\n",
      "Epoch 570 is train loss: 0.09333946812377365, train accuracy: 0.9583333333333334\n",
      "Epoch 575 is train loss: 0.0920126284492747, train accuracy: 0.9583333333333334\n",
      "Epoch 580 is train loss: 0.09076793113346153, train accuracy: 0.9583333333333334\n",
      "Epoch 585 is train loss: 0.08959837408749309, train accuracy: 0.9583333333333334\n",
      "Epoch 590 is train loss: 0.08849768572685252, train accuracy: 0.9583333333333334\n",
      "Epoch 595 is train loss: 0.0874602361606308, train accuracy: 0.9583333333333334\n",
      "Epoch 600 is train loss: 0.08648096059377455, train accuracy: 0.9583333333333334\n",
      "Epoch 605 is train loss: 0.08555529307039712, train accuracy: 0.9666666666666667\n",
      "Epoch 610 is train loss: 0.08467910900524506, train accuracy: 0.9666666666666667\n",
      "Epoch 615 is train loss: 0.0838486752084407, train accuracy: 0.9666666666666667\n",
      "Epoch 620 is train loss: 0.08306060631895179, train accuracy: 0.9666666666666667\n",
      "Epoch 625 is train loss: 0.08231182673481241, train accuracy: 0.9666666666666667\n",
      "Epoch 630 is train loss: 0.08159953727051363, train accuracy: 0.975\n",
      "Epoch 635 is train loss: 0.08092118589004892, train accuracy: 0.975\n",
      "Epoch 640 is train loss: 0.08027444196241484, train accuracy: 0.975\n",
      "Epoch 645 is train loss: 0.07965717356854192, train accuracy: 0.975\n",
      "Epoch 650 is train loss: 0.07906742745755828, train accuracy: 0.975\n",
      "Epoch 655 is train loss: 0.07850341130827415, train accuracy: 0.975\n",
      "Epoch 660 is train loss: 0.07796347800070567, train accuracy: 0.975\n",
      "Epoch 665 is train loss: 0.07744611164384968, train accuracy: 0.975\n",
      "Epoch 670 is train loss: 0.07694991514103357, train accuracy: 0.975\n",
      "Epoch 675 is train loss: 0.07647359910401356, train accuracy: 0.975\n",
      "Epoch 680 is train loss: 0.07601597195243374, train accuracy: 0.975\n",
      "Epoch 685 is train loss: 0.07557593105698587, train accuracy: 0.975\n",
      "Epoch 690 is train loss: 0.07515245480320426, train accuracy: 0.975\n",
      "Epoch 695 is train loss: 0.07474459546877964, train accuracy: 0.975\n",
      "Epoch 700 is train loss: 0.0743514728209837, train accuracy: 0.975\n",
      "Trainig is done:\n",
      "the training loss is 0.5564805739787807 and the training accuracy is 0.975\n",
      "the testing loss is 0.14608061051997898 and the testing accuracy is 0.9333333333333333\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot of the confusion matrix, where the rows represent the real data and the columns the predicted data:\n",
      "                   Iris-versicolor\\n  Iris-setosa\\n  Iris-virginica\\n\n",
      "Iris-versicolor\\n               12.0            0.0               2.0\n",
      "Iris-setosa\\n                    0.0            8.0               0.0\n",
      "Iris-virginica\\n                 0.0            0.0               8.0\n"
     ]
    }
   ],
   "source": [
    "myMlp.train(data, labels, 700, 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se prueba la segunda estructura: una red de 5 capas ocultas, 7 unidades por capa:\n",
    "Esta red se entrena con 700 epochs y 0.02 de learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "myMlp2 = MLP(data, labels, n_layers = 3, n_hidden_units=15, named_labels=named_classes, activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 is train loss: 2.331942373399816, train accuracy: 0.5333333333333333\n",
      "Epoch 10 is train loss: 1.089495090049982, train accuracy: 0.5166666666666667\n",
      "Epoch 15 is train loss: 0.9308587843234728, train accuracy: 0.49166666666666664\n",
      "Epoch 20 is train loss: 0.9082054123782721, train accuracy: 0.49166666666666664\n",
      "Epoch 25 is train loss: 0.8996587058338935, train accuracy: 0.5\n",
      "Epoch 30 is train loss: 0.8930862283360298, train accuracy: 0.5083333333333333\n",
      "Epoch 35 is train loss: 0.8874126705275757, train accuracy: 0.5083333333333333\n",
      "Epoch 40 is train loss: 0.8823906874624204, train accuracy: 0.5083333333333333\n",
      "Epoch 45 is train loss: 0.8777875129704223, train accuracy: 0.5083333333333333\n",
      "Epoch 50 is train loss: 0.8734372434943665, train accuracy: 0.5083333333333333\n",
      "Epoch 55 is train loss: 0.8695442181112415, train accuracy: 0.5083333333333333\n",
      "Epoch 60 is train loss: 0.8659355889412409, train accuracy: 0.5083333333333333\n",
      "Epoch 65 is train loss: 0.8626031414229764, train accuracy: 0.5083333333333333\n",
      "Epoch 70 is train loss: 0.8594630363500748, train accuracy: 0.5166666666666667\n",
      "Epoch 75 is train loss: 0.8562215425939298, train accuracy: 0.5166666666666667\n",
      "Epoch 80 is train loss: 0.8528480917468425, train accuracy: 0.5166666666666667\n",
      "Epoch 85 is train loss: 0.8494972576989869, train accuracy: 0.5416666666666666\n",
      "Epoch 90 is train loss: 0.8458693082098383, train accuracy: 0.5416666666666666\n",
      "Epoch 95 is train loss: 0.8418308465169794, train accuracy: 0.55\n",
      "Epoch 100 is train loss: 0.8373762066992463, train accuracy: 0.55\n",
      "Epoch 105 is train loss: 0.8326595314678371, train accuracy: 0.55\n",
      "Epoch 110 is train loss: 0.8279090347082487, train accuracy: 0.55\n",
      "Epoch 115 is train loss: 0.8228920581249716, train accuracy: 0.5583333333333333\n",
      "Epoch 120 is train loss: 0.8172357815523168, train accuracy: 0.5583333333333333\n",
      "Epoch 125 is train loss: 0.8113625626587345, train accuracy: 0.5666666666666667\n",
      "Epoch 130 is train loss: 0.8062178002474589, train accuracy: 0.5666666666666667\n",
      "Epoch 135 is train loss: 0.800955407501133, train accuracy: 0.5833333333333334\n",
      "Epoch 140 is train loss: 0.7950494936514959, train accuracy: 0.5833333333333334\n",
      "Epoch 145 is train loss: 0.7889350584987164, train accuracy: 0.5833333333333334\n",
      "Epoch 150 is train loss: 0.7812181054731607, train accuracy: 0.5916666666666667\n",
      "Epoch 155 is train loss: 0.7724092424486994, train accuracy: 0.6083333333333333\n",
      "Epoch 160 is train loss: 0.7630937623041312, train accuracy: 0.625\n",
      "Epoch 165 is train loss: 0.7539150001476483, train accuracy: 0.6333333333333333\n",
      "Epoch 170 is train loss: 0.7450979354737454, train accuracy: 0.6416666666666667\n",
      "Epoch 175 is train loss: 0.7354930196009338, train accuracy: 0.6416666666666667\n",
      "Epoch 180 is train loss: 0.7259747175866684, train accuracy: 0.65\n",
      "Epoch 185 is train loss: 0.717961200237062, train accuracy: 0.6833333333333333\n",
      "Epoch 190 is train loss: 0.7095024268083622, train accuracy: 0.6833333333333333\n",
      "Epoch 195 is train loss: 0.7011167382823189, train accuracy: 0.675\n",
      "Epoch 200 is train loss: 0.6940932408730327, train accuracy: 0.675\n",
      "Epoch 205 is train loss: 0.688126477078914, train accuracy: 0.675\n",
      "Epoch 210 is train loss: 0.6823826279791834, train accuracy: 0.6833333333333333\n",
      "Epoch 215 is train loss: 0.6765347644612264, train accuracy: 0.6833333333333333\n",
      "Epoch 220 is train loss: 0.6707248976830963, train accuracy: 0.6833333333333333\n",
      "Epoch 225 is train loss: 0.6652252730090242, train accuracy: 0.6916666666666667\n",
      "Epoch 230 is train loss: 0.659984043850262, train accuracy: 0.7\n",
      "Epoch 235 is train loss: 0.6548918872417401, train accuracy: 0.7\n",
      "Epoch 240 is train loss: 0.649924858731858, train accuracy: 0.7\n",
      "Epoch 245 is train loss: 0.6452110708068033, train accuracy: 0.7\n",
      "Epoch 250 is train loss: 0.6403533153198052, train accuracy: 0.7083333333333334\n",
      "Epoch 255 is train loss: 0.6356128859758152, train accuracy: 0.7166666666666667\n",
      "Epoch 260 is train loss: 0.6312548940394422, train accuracy: 0.7166666666666667\n",
      "Epoch 265 is train loss: 0.6270919468832382, train accuracy: 0.7333333333333333\n",
      "Epoch 270 is train loss: 0.622945103412722, train accuracy: 0.7333333333333333\n",
      "Epoch 275 is train loss: 0.6188565319733121, train accuracy: 0.7333333333333333\n",
      "Epoch 280 is train loss: 0.6147990082648084, train accuracy: 0.7333333333333333\n",
      "Epoch 285 is train loss: 0.6107184086394685, train accuracy: 0.7416666666666667\n",
      "Epoch 290 is train loss: 0.6066765673583507, train accuracy: 0.7416666666666667\n",
      "Epoch 295 is train loss: 0.6027006867843466, train accuracy: 0.7416666666666667\n",
      "Epoch 300 is train loss: 0.5987068762793697, train accuracy: 0.75\n",
      "Epoch 305 is train loss: 0.5947433969352852, train accuracy: 0.75\n",
      "Epoch 310 is train loss: 0.5907118910287584, train accuracy: 0.75\n",
      "Epoch 315 is train loss: 0.5866662491534863, train accuracy: 0.75\n",
      "Epoch 320 is train loss: 0.5826328441452929, train accuracy: 0.75\n",
      "Epoch 325 is train loss: 0.5785772012519331, train accuracy: 0.7583333333333333\n",
      "Epoch 330 is train loss: 0.5745864295185334, train accuracy: 0.7583333333333333\n",
      "Epoch 335 is train loss: 0.5704994805537543, train accuracy: 0.7666666666666667\n",
      "Epoch 340 is train loss: 0.5664990401678077, train accuracy: 0.7583333333333333\n",
      "Epoch 345 is train loss: 0.5627409850911768, train accuracy: 0.7583333333333333\n",
      "Epoch 350 is train loss: 0.5590052605915484, train accuracy: 0.7583333333333333\n",
      "Epoch 355 is train loss: 0.5553759755631165, train accuracy: 0.7666666666666667\n",
      "Epoch 360 is train loss: 0.5512999511630038, train accuracy: 0.7583333333333333\n",
      "Epoch 365 is train loss: 0.5475052676253546, train accuracy: 0.7833333333333333\n",
      "Epoch 370 is train loss: 0.5440281725432494, train accuracy: 0.7583333333333333\n",
      "Epoch 375 is train loss: 0.5404691580923071, train accuracy: 0.7833333333333333\n",
      "Epoch 380 is train loss: 0.5368524733820573, train accuracy: 0.7583333333333333\n",
      "Epoch 385 is train loss: 0.5335122877324227, train accuracy: 0.7916666666666666\n",
      "Epoch 390 is train loss: 0.530142819672896, train accuracy: 0.7583333333333333\n",
      "Epoch 395 is train loss: 0.5265888541471665, train accuracy: 0.7916666666666666\n",
      "Epoch 400 is train loss: 0.5233989072006353, train accuracy: 0.7583333333333333\n",
      "Epoch 405 is train loss: 0.5197832725643783, train accuracy: 0.775\n",
      "Epoch 410 is train loss: 0.5165893141949895, train accuracy: 0.7916666666666666\n",
      "Epoch 415 is train loss: 0.5133127905724627, train accuracy: 0.7666666666666667\n",
      "Epoch 420 is train loss: 0.5098830181873707, train accuracy: 0.7916666666666666\n",
      "Epoch 425 is train loss: 0.5072019278929134, train accuracy: 0.7666666666666667\n",
      "Epoch 430 is train loss: 0.5034374770285078, train accuracy: 0.7833333333333333\n",
      "Epoch 435 is train loss: 0.5003312553423028, train accuracy: 0.7916666666666666\n",
      "Epoch 440 is train loss: 0.4974344064460679, train accuracy: 0.7916666666666666\n",
      "Epoch 445 is train loss: 0.49415138897578476, train accuracy: 0.7916666666666666\n",
      "Epoch 450 is train loss: 0.490872423557906, train accuracy: 0.7833333333333333\n",
      "Epoch 455 is train loss: 0.4879305373787217, train accuracy: 0.7916666666666666\n",
      "Epoch 460 is train loss: 0.48495783387624986, train accuracy: 0.7916666666666666\n",
      "Epoch 465 is train loss: 0.4817876564317409, train accuracy: 0.7833333333333333\n",
      "Epoch 470 is train loss: 0.4789266162060436, train accuracy: 0.7916666666666666\n",
      "Epoch 475 is train loss: 0.4759552828077687, train accuracy: 0.7833333333333333\n",
      "Epoch 480 is train loss: 0.47326636787360177, train accuracy: 0.825\n",
      "Epoch 485 is train loss: 0.47047205763942374, train accuracy: 0.8\n",
      "Epoch 490 is train loss: 0.4671975415356056, train accuracy: 0.825\n",
      "Epoch 495 is train loss: 0.46399679535960076, train accuracy: 0.7916666666666666\n",
      "Epoch 500 is train loss: 0.4609581473043194, train accuracy: 0.825\n",
      "Epoch 505 is train loss: 0.4583191692851189, train accuracy: 0.8\n",
      "Epoch 510 is train loss: 0.4549637283567929, train accuracy: 0.825\n",
      "Epoch 515 is train loss: 0.4521702928585369, train accuracy: 0.8\n",
      "Epoch 520 is train loss: 0.44955519706535696, train accuracy: 0.825\n",
      "Epoch 525 is train loss: 0.4465271823782497, train accuracy: 0.8166666666666667\n",
      "Epoch 530 is train loss: 0.4440374537743702, train accuracy: 0.825\n",
      "Epoch 535 is train loss: 0.4408974513605618, train accuracy: 0.8166666666666667\n",
      "Epoch 540 is train loss: 0.4379536367263687, train accuracy: 0.825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 545 is train loss: 0.43508942893403807, train accuracy: 0.8166666666666667\n",
      "Epoch 550 is train loss: 0.43233817441980676, train accuracy: 0.8416666666666667\n",
      "Epoch 555 is train loss: 0.4301922559790635, train accuracy: 0.8166666666666667\n",
      "Epoch 560 is train loss: 0.4275715570578612, train accuracy: 0.8333333333333334\n",
      "Epoch 565 is train loss: 0.4246977958746927, train accuracy: 0.8166666666666667\n",
      "Epoch 570 is train loss: 0.42224275829748176, train accuracy: 0.825\n",
      "Epoch 575 is train loss: 0.41885768222989384, train accuracy: 0.85\n",
      "Epoch 580 is train loss: 0.4172159643019132, train accuracy: 0.825\n",
      "Epoch 585 is train loss: 0.41410063194311847, train accuracy: 0.85\n",
      "Epoch 590 is train loss: 0.4127065259070097, train accuracy: 0.825\n",
      "Epoch 595 is train loss: 0.4126202827818942, train accuracy: 0.8083333333333333\n",
      "Epoch 600 is train loss: 0.40709245886915185, train accuracy: 0.8416666666666667\n",
      "Epoch 605 is train loss: 0.40695822851339564, train accuracy: 0.8166666666666667\n",
      "Epoch 610 is train loss: 0.40572576364867236, train accuracy: 0.825\n",
      "Epoch 615 is train loss: 0.40124059035404713, train accuracy: 0.85\n",
      "Epoch 620 is train loss: 0.4035693434001016, train accuracy: 0.825\n",
      "Epoch 625 is train loss: 0.4005864545683771, train accuracy: 0.8166666666666667\n",
      "Epoch 630 is train loss: 0.39606055890935915, train accuracy: 0.8333333333333334\n",
      "Epoch 635 is train loss: 0.3964343171179286, train accuracy: 0.8166666666666667\n",
      "Epoch 640 is train loss: 0.3993435475785583, train accuracy: 0.8166666666666667\n",
      "Epoch 645 is train loss: 0.3899672143817, train accuracy: 0.8583333333333333\n",
      "Epoch 650 is train loss: 0.3917074368041628, train accuracy: 0.8333333333333334\n",
      "Epoch 655 is train loss: 0.3910496947741932, train accuracy: 0.8166666666666667\n",
      "Epoch 660 is train loss: 0.3855401065367091, train accuracy: 0.8416666666666667\n",
      "Epoch 665 is train loss: 0.38895407357711775, train accuracy: 0.8083333333333333\n",
      "Epoch 670 is train loss: 0.38699702907189376, train accuracy: 0.8416666666666667\n",
      "Epoch 675 is train loss: 0.3810503400549553, train accuracy: 0.8583333333333333\n",
      "Epoch 680 is train loss: 0.3819337154422954, train accuracy: 0.8333333333333334\n",
      "Epoch 685 is train loss: 0.3802672757772457, train accuracy: 0.85\n",
      "Epoch 690 is train loss: 0.3762754156872361, train accuracy: 0.85\n",
      "Epoch 695 is train loss: 0.3820508477157001, train accuracy: 0.8083333333333333\n",
      "Epoch 700 is train loss: 0.37747092857504133, train accuracy: 0.8333333333333334\n",
      "Trainig is done:\n",
      "the training loss is 0.6094073937052773 and the training accuracy is 0.8333333333333334\n",
      "the testing loss is 0.37017124783739763 and the testing accuracy is 0.9333333333333333\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot of the confusion matrix, where the rows represent the real data and the columns the predicted data:\n",
      "                   Iris-versicolor\\n  Iris-setosa\\n  Iris-virginica\\n\n",
      "Iris-versicolor\\n                6.0            0.0               0.0\n",
      "Iris-setosa\\n                    1.0           11.0               1.0\n",
      "Iris-virginica\\n                 0.0            0.0              11.0\n"
     ]
    }
   ],
   "source": [
    "myMlp2.train(data, labels, 700, 0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba con otro dataset:\n",
    "se hará la prueba con el dataset seeds entregado en la tarea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se cargan los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature data shape (210, 7)\n",
      "labels of data shape (210, 3)\n"
     ]
    }
   ],
   "source": [
    "#acá se define el nombre del archivo que se procede a trabajar:\n",
    "file_name = 'seeds_dataset.txt'\n",
    "#generamos listas para guardar los elementos del archivo de datos\n",
    "labels = list()\n",
    "data = list()\n",
    "one_hot_classes = dict()\n",
    "#Una vez llenas estas listas se procederá a pasar esta a un arreglo de numpy como se explicó en el apartado anterior:\n",
    "# abrimos el archivo file_name:\n",
    "with open(file_name) as file: \n",
    "    #procedemos a acceder a todas las filas del archivo:\n",
    "    for line in file: \n",
    "        #dividimos el archivo por comas donde sabemos que el último elemento es el label\n",
    "        splited_line = line.split()\n",
    "        #Pasamos a checkear si el arreglo queda vacio o no:\n",
    "        if splited_line != ['\\n']:\n",
    "            #Unimos las listas con los nuevos elementos\n",
    "            # el elemento [-1] indica que se accede al último elemento del arreglo\n",
    "            labels.append(splited_line[-1])\n",
    "            #acá accedemos a todos los elementos salvo el último.\n",
    "            data.append(splited_line[:-1])\n",
    "            \n",
    "#set de valores (valores sin repetir de la lista)\n",
    "classification_set = set(labels)\n",
    "n_C = len(classification_set)\n",
    "\n",
    "# se genera un loop donde enumerate entrega los elementos contados {0,1,2} \n",
    "# y para cada elemento en el set de labels (que son solamente 3 valores) se genera un one-hot encoding\n",
    "for index, value in enumerate(classification_set):\n",
    "    zero = np.zeros((1, n_C))\n",
    "    if index == 0:\n",
    "        zero[:,index] = 1\n",
    "        one_hot_classes[value]= zero \n",
    "    else:\n",
    "        zero[:,index] = 1\n",
    "        one_hot_classes[value]= zero\n",
    "\n",
    "#Procedemos a calcular el one-hot encoding de todos los labels.:\n",
    "for i, example in enumerate(labels):\n",
    "    labels[i] = one_hot_classes[example]\n",
    "\n",
    "#vstack es un método de la librería de numpy la que toma dos listas y las apila como filas. De este modo,\n",
    "# si tenemos N filas de M elementos este nos entrega una función de (N,M).\n",
    "data = np.vstack(data).astype('float64')\n",
    "labels = np.vstack(labels)\n",
    "#como el dataset viene ordenado es necesario desordenarlo para que los ejemplos no se encuentren todos juntos.\n",
    "# de este modo, permutation de numpy permite entregar un arreglo de indices desordenados que permite que el arreglo quede\n",
    "#desordenado.\n",
    "permutation = np.random.permutation(data.shape[0])\n",
    "data2 = data[permutation,:]\n",
    "labels2 = labels[permutation]\n",
    "#print the shape of data and labels:\n",
    "print('feature data shape '+ str(data.shape))\n",
    "print('labels of data shape ' + str(labels.shape))\n",
    "\n",
    "data_max = np.max(data2, axis=0)\n",
    "data_min = np.min(data2, axis=0)\n",
    "data2 = (data2-data_min)/(data_max-data_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se genera una red con 3 capas ocultas de 20 unidades cada una"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "myMlp3 = MLP(data2, labels2, n_layers = 4, n_hidden_units=20, named_labels=classification_set, activation='tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 is train loss: 5.338312762036959, train accuracy: 0.3333333333333333\n",
      "Epoch 10 is train loss: 6.441188765923265, train accuracy: 0.36904761904761907\n",
      "Epoch 15 is train loss: 4.252421990223499, train accuracy: 0.2976190476190476\n",
      "Epoch 20 is train loss: 8.544314779460706, train accuracy: 0.3333333333333333\n",
      "Epoch 25 is train loss: 6.465779010955952, train accuracy: 0.36904761904761907\n",
      "Epoch 30 is train loss: 4.961600745744508, train accuracy: 0.2976190476190476\n",
      "Epoch 35 is train loss: 8.485743361146595, train accuracy: 0.3333333333333333\n",
      "Epoch 40 is train loss: 5.313903211865532, train accuracy: 0.36904761904761907\n",
      "Epoch 45 is train loss: 4.1727211274923555, train accuracy: 0.2976190476190476\n",
      "Epoch 50 is train loss: 4.328113746631579, train accuracy: 0.3333333333333333\n",
      "Epoch 55 is train loss: 2.1551880240701684, train accuracy: 0.36904761904761907\n",
      "Epoch 60 is train loss: 1.3451352848408706, train accuracy: 0.43452380952380953\n",
      "Epoch 65 is train loss: 1.296555469799593, train accuracy: 0.6309523809523809\n",
      "Epoch 70 is train loss: 1.111640343700272, train accuracy: 0.5595238095238095\n",
      "Epoch 75 is train loss: 1.0868258703065505, train accuracy: 0.625\n",
      "Epoch 80 is train loss: 0.8219863582607894, train accuracy: 0.5535714285714286\n",
      "Epoch 85 is train loss: 0.8463844833157649, train accuracy: 0.6309523809523809\n",
      "Epoch 90 is train loss: 0.6696462899303055, train accuracy: 0.6845238095238095\n",
      "Epoch 95 is train loss: 0.6665114409709574, train accuracy: 0.6428571428571429\n",
      "Epoch 100 is train loss: 0.5812987568666619, train accuracy: 0.7857142857142857\n",
      "Epoch 105 is train loss: 0.5599686394393945, train accuracy: 0.7678571428571429\n",
      "Epoch 110 is train loss: 0.526340050217418, train accuracy: 0.8154761904761905\n",
      "Epoch 115 is train loss: 0.5109618240106869, train accuracy: 0.8273809523809523\n",
      "Epoch 120 is train loss: 0.5001772676477527, train accuracy: 0.8273809523809523\n",
      "Epoch 125 is train loss: 0.4934495016279972, train accuracy: 0.8214285714285714\n",
      "Epoch 130 is train loss: 0.4881706060883933, train accuracy: 0.8154761904761905\n",
      "Epoch 135 is train loss: 0.48359227218435225, train accuracy: 0.8154761904761905\n",
      "Epoch 140 is train loss: 0.4794997158480673, train accuracy: 0.8154761904761905\n",
      "Epoch 145 is train loss: 0.4758146679066834, train accuracy: 0.8154761904761905\n",
      "Epoch 150 is train loss: 0.4724831659514797, train accuracy: 0.8214285714285714\n",
      "Epoch 155 is train loss: 0.46946050652799914, train accuracy: 0.8214285714285714\n",
      "Epoch 160 is train loss: 0.46670885051892175, train accuracy: 0.8214285714285714\n",
      "Epoch 165 is train loss: 0.4641959872796797, train accuracy: 0.8214285714285714\n",
      "Epoch 170 is train loss: 0.4618943709421787, train accuracy: 0.8214285714285714\n",
      "Epoch 175 is train loss: 0.45978034043415555, train accuracy: 0.8214285714285714\n",
      "Epoch 180 is train loss: 0.45783348444220595, train accuracy: 0.8214285714285714\n",
      "Epoch 185 is train loss: 0.4560361218029615, train accuracy: 0.8214285714285714\n",
      "Epoch 190 is train loss: 0.4543728744333868, train accuracy: 0.8214285714285714\n",
      "Epoch 195 is train loss: 0.45283031487529046, train accuracy: 0.8214285714285714\n",
      "Epoch 200 is train loss: 0.4513966743063295, train accuracy: 0.8214285714285714\n",
      "Epoch 205 is train loss: 0.45006159976851684, train accuracy: 0.8214285714285714\n",
      "Epoch 210 is train loss: 0.4488159516117759, train accuracy: 0.8214285714285714\n",
      "Epoch 215 is train loss: 0.4476516339079784, train accuracy: 0.8214285714285714\n",
      "Epoch 220 is train loss: 0.446561451978696, train accuracy: 0.8214285714285714\n",
      "Epoch 225 is train loss: 0.4455389922840281, train accuracy: 0.8214285714285714\n",
      "Epoch 230 is train loss: 0.4445785208039993, train accuracy: 0.8214285714285714\n",
      "Epoch 235 is train loss: 0.44367489675565347, train accuracy: 0.8214285714285714\n",
      "Epoch 240 is train loss: 0.4428234990640163, train accuracy: 0.8214285714285714\n",
      "Epoch 245 is train loss: 0.44202016347116463, train accuracy: 0.8214285714285714\n",
      "Epoch 250 is train loss: 0.44126112854617844, train accuracy: 0.8273809523809523\n",
      "Epoch 255 is train loss: 0.44054298916670426, train accuracy: 0.8273809523809523\n",
      "Epoch 260 is train loss: 0.4398626562936844, train accuracy: 0.8273809523809523\n",
      "Epoch 265 is train loss: 0.43921732206533126, train accuracy: 0.8273809523809523\n",
      "Epoch 270 is train loss: 0.4386044294033703, train accuracy: 0.8273809523809523\n",
      "Epoch 275 is train loss: 0.43802164546100025, train accuracy: 0.8273809523809523\n",
      "Epoch 280 is train loss: 0.4374668383536712, train accuracy: 0.8273809523809523\n",
      "Epoch 285 is train loss: 0.4369380567053061, train accuracy: 0.8273809523809523\n",
      "Epoch 290 is train loss: 0.43643351161777216, train accuracy: 0.8273809523809523\n",
      "Epoch 295 is train loss: 0.4359515607333083, train accuracy: 0.8273809523809523\n",
      "Epoch 300 is train loss: 0.43549069411072544, train accuracy: 0.8273809523809523\n",
      "Epoch 305 is train loss: 0.43504952167851046, train accuracy: 0.8273809523809523\n",
      "Epoch 310 is train loss: 0.43462676206313505, train accuracy: 0.8273809523809523\n",
      "Epoch 315 is train loss: 0.4342212326201767, train accuracy: 0.8273809523809523\n",
      "Epoch 320 is train loss: 0.4338318405203978, train accuracy: 0.8273809523809523\n",
      "Epoch 325 is train loss: 0.4334575747635307, train accuracy: 0.8273809523809523\n",
      "Epoch 330 is train loss: 0.4330974990098898, train accuracy: 0.8273809523809523\n",
      "Epoch 335 is train loss: 0.4327507451346364, train accuracy: 0.8273809523809523\n",
      "Epoch 340 is train loss: 0.43241650742201265, train accuracy: 0.8273809523809523\n",
      "Epoch 345 is train loss: 0.4320940373275107, train accuracy: 0.8273809523809523\n",
      "Epoch 350 is train loss: 0.43178263874505307, train accuracy: 0.8273809523809523\n",
      "Epoch 355 is train loss: 0.4314816637240804, train accuracy: 0.8273809523809523\n",
      "Epoch 360 is train loss: 0.4311905085881823, train accuracy: 0.8273809523809523\n",
      "Epoch 365 is train loss: 0.43090861041271705, train accuracy: 0.8273809523809523\n",
      "Epoch 370 is train loss: 0.4306354438239097, train accuracy: 0.8273809523809523\n",
      "Epoch 375 is train loss: 0.4303705180862914, train accuracy: 0.8273809523809523\n",
      "Epoch 380 is train loss: 0.4301133744491528, train accuracy: 0.8273809523809523\n",
      "Epoch 385 is train loss: 0.4298635837260088, train accuracy: 0.8273809523809523\n",
      "Epoch 390 is train loss: 0.42962074408398093, train accuracy: 0.8273809523809523\n",
      "Epoch 395 is train loss: 0.4293844790225518, train accuracy: 0.8273809523809523\n",
      "Epoch 400 is train loss: 0.42915443552338417, train accuracy: 0.8273809523809523\n",
      "Epoch 405 is train loss: 0.42893028235486697, train accuracy: 0.8273809523809523\n",
      "Epoch 410 is train loss: 0.42871170851678675, train accuracy: 0.8273809523809523\n",
      "Epoch 415 is train loss: 0.42849842181205816, train accuracy: 0.8273809523809523\n",
      "Epoch 420 is train loss: 0.42829014753379946, train accuracy: 0.8273809523809523\n",
      "Epoch 425 is train loss: 0.428086627257241, train accuracy: 0.8273809523809523\n",
      "Epoch 430 is train loss: 0.42788761772702444, train accuracy: 0.8273809523809523\n",
      "Epoch 435 is train loss: 0.42769288983139087, train accuracy: 0.8273809523809523\n",
      "Epoch 440 is train loss: 0.42750222765560475, train accuracy: 0.8273809523809523\n",
      "Epoch 445 is train loss: 0.4273154276077045, train accuracy: 0.8273809523809523\n",
      "Epoch 450 is train loss: 0.4271322976103485, train accuracy: 0.8273809523809523\n",
      "Epoch 455 is train loss: 0.4269526563531156, train accuracy: 0.8273809523809523\n",
      "Epoch 460 is train loss: 0.42677633260016, train accuracy: 0.8273809523809523\n",
      "Epoch 465 is train loss: 0.4266031645485973, train accuracy: 0.8273809523809523\n",
      "Epoch 470 is train loss: 0.4264329992334285, train accuracy: 0.8273809523809523\n",
      "Epoch 475 is train loss: 0.426265691975195, train accuracy: 0.8273809523809523\n",
      "Epoch 480 is train loss: 0.4261011058669035, train accuracy: 0.8273809523809523\n",
      "Epoch 485 is train loss: 0.42593911129707623, train accuracy: 0.8273809523809523\n",
      "Epoch 490 is train loss: 0.4257795855060522, train accuracy: 0.8273809523809523\n",
      "Epoch 495 is train loss: 0.42562241217293345, train accuracy: 0.8273809523809523\n",
      "Epoch 500 is train loss: 0.42546748103078647, train accuracy: 0.8273809523809523\n",
      "Epoch 505 is train loss: 0.42531468750792245, train accuracy: 0.8273809523809523\n",
      "Epoch 510 is train loss: 0.4251639323932645, train accuracy: 0.8273809523809523\n",
      "Epoch 515 is train loss: 0.4250151215239817, train accuracy: 0.8273809523809523\n",
      "Epoch 520 is train loss: 0.424868165493719, train accuracy: 0.8273809523809523\n",
      "Epoch 525 is train loss: 0.4247229793798961, train accuracy: 0.8273809523809523\n",
      "Epoch 530 is train loss: 0.42457948248867305, train accuracy: 0.8273809523809523\n",
      "Epoch 535 is train loss: 0.4244375981162918, train accuracy: 0.8273809523809523\n",
      "Epoch 540 is train loss: 0.4242972533256149, train accuracy: 0.8273809523809523\n",
      "Epoch 545 is train loss: 0.42415837873677004, train accuracy: 0.8273809523809523\n",
      "Epoch 550 is train loss: 0.42402090833090067, train accuracy: 0.8273809523809523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 555 is train loss: 0.42388477926610096, train accuracy: 0.8273809523809523\n",
      "Epoch 560 is train loss: 0.4237499317046846, train accuracy: 0.8273809523809523\n",
      "Epoch 565 is train loss: 0.42361630865100386, train accuracy: 0.8273809523809523\n",
      "Epoch 570 is train loss: 0.4234838557990964, train accuracy: 0.8273809523809523\n",
      "Epoch 575 is train loss: 0.4233525213894903, train accuracy: 0.8273809523809523\n",
      "Epoch 580 is train loss: 0.4232222560745503, train accuracy: 0.8273809523809523\n",
      "Epoch 585 is train loss: 0.4230930127917941, train accuracy: 0.8273809523809523\n",
      "Epoch 590 is train loss: 0.4229647466446507, train accuracy: 0.8273809523809523\n",
      "Epoch 595 is train loss: 0.4228374147901697, train accuracy: 0.8273809523809523\n",
      "Epoch 600 is train loss: 0.4227109763332283, train accuracy: 0.8273809523809523\n",
      "Epoch 605 is train loss: 0.4225853922268157, train accuracy: 0.8273809523809523\n",
      "Epoch 610 is train loss: 0.4224606251780044, train accuracy: 0.8273809523809523\n",
      "Epoch 615 is train loss: 0.4223366395592443, train accuracy: 0.8273809523809523\n",
      "Epoch 620 is train loss: 0.4222134013246444, train accuracy: 0.8273809523809523\n",
      "Epoch 625 is train loss: 0.42209087793092853, train accuracy: 0.8273809523809523\n",
      "Epoch 630 is train loss: 0.4219690382627731, train accuracy: 0.8273809523809523\n",
      "Epoch 635 is train loss: 0.42184785256225676, train accuracy: 0.8273809523809523\n",
      "Epoch 640 is train loss: 0.42172729236216727, train accuracy: 0.8273809523809523\n",
      "Epoch 645 is train loss: 0.42160733042293463, train accuracy: 0.8273809523809523\n",
      "Epoch 650 is train loss: 0.4214879406729657, train accuracy: 0.8273809523809523\n",
      "Epoch 655 is train loss: 0.4213690981521799, train accuracy: 0.8273809523809523\n",
      "Epoch 660 is train loss: 0.42125077895855345, train accuracy: 0.8392857142857143\n",
      "Epoch 665 is train loss: 0.42113296019749324, train accuracy: 0.8392857142857143\n",
      "Epoch 670 is train loss: 0.4210156199338752, train accuracy: 0.8392857142857143\n",
      "Epoch 675 is train loss: 0.42089873714659054, train accuracy: 0.8392857142857143\n",
      "Epoch 680 is train loss: 0.4207822916854537, train accuracy: 0.8392857142857143\n",
      "Epoch 685 is train loss: 0.42066626423033754, train accuracy: 0.8392857142857143\n",
      "Epoch 690 is train loss: 0.4205506362524067, train accuracy: 0.8392857142857143\n",
      "Epoch 695 is train loss: 0.42043538997732943, train accuracy: 0.8392857142857143\n",
      "Epoch 700 is train loss: 0.42032050835035806, train accuracy: 0.8392857142857143\n",
      "Epoch 705 is train loss: 0.4202059750031707, train accuracy: 0.8392857142857143\n",
      "Epoch 710 is train loss: 0.4200917742223775, train accuracy: 0.8392857142857143\n",
      "Epoch 715 is train loss: 0.41997789091959786, train accuracy: 0.8392857142857143\n",
      "Epoch 720 is train loss: 0.4198643106030237, train accuracy: 0.8392857142857143\n",
      "Epoch 725 is train loss: 0.419751019350385, train accuracy: 0.8392857142857143\n",
      "Epoch 730 is train loss: 0.4196380037832436, train accuracy: 0.8392857142857143\n",
      "Epoch 735 is train loss: 0.4195252510425417, train accuracy: 0.8392857142857143\n",
      "Epoch 740 is train loss: 0.419412748765338, train accuracy: 0.8392857142857143\n",
      "Epoch 745 is train loss: 0.4193004850626688, train accuracy: 0.8392857142857143\n",
      "Epoch 750 is train loss: 0.41918844849847253, train accuracy: 0.8392857142857143\n",
      "Epoch 755 is train loss: 0.41907662806952206, train accuracy: 0.8392857142857143\n",
      "Epoch 760 is train loss: 0.4189650131863144, train accuracy: 0.8392857142857143\n",
      "Epoch 765 is train loss: 0.4188535936548622, train accuracy: 0.8392857142857143\n",
      "Epoch 770 is train loss: 0.4187423596593459, train accuracy: 0.8392857142857143\n",
      "Epoch 775 is train loss: 0.4186313017455782, train accuracy: 0.8392857142857143\n",
      "Epoch 780 is train loss: 0.41852041080524166, train accuracy: 0.8392857142857143\n",
      "Epoch 785 is train loss: 0.4184096780608556, train accuracy: 0.8392857142857143\n",
      "Epoch 790 is train loss: 0.4182990950514427, train accuracy: 0.8392857142857143\n",
      "Epoch 795 is train loss: 0.41818865361885116, train accuracy: 0.8392857142857143\n",
      "Epoch 800 is train loss: 0.41807834589470566, train accuracy: 0.8392857142857143\n",
      "Epoch 805 is train loss: 0.4179681642879536, train accuracy: 0.8392857142857143\n",
      "Epoch 810 is train loss: 0.41785810147297553, train accuracy: 0.8333333333333334\n",
      "Epoch 815 is train loss: 0.417748150378235, train accuracy: 0.8333333333333334\n",
      "Epoch 820 is train loss: 0.4176383041754369, train accuracy: 0.8333333333333334\n",
      "Epoch 825 is train loss: 0.41752855626917434, train accuracy: 0.8333333333333334\n",
      "Epoch 830 is train loss: 0.4174189002870353, train accuracy: 0.8333333333333334\n",
      "Epoch 835 is train loss: 0.41730933007015075, train accuracy: 0.8333333333333334\n",
      "Epoch 840 is train loss: 0.41719983966415924, train accuracy: 0.8333333333333334\n",
      "Epoch 845 is train loss: 0.41709042331057156, train accuracy: 0.8333333333333334\n",
      "Epoch 850 is train loss: 0.4169810754385131, train accuracy: 0.8333333333333334\n",
      "Epoch 855 is train loss: 0.4168717906568267, train accuracy: 0.8333333333333334\n",
      "Epoch 860 is train loss: 0.4167625637465222, train accuracy: 0.8333333333333334\n",
      "Epoch 865 is train loss: 0.4166533896535488, train accuracy: 0.8333333333333334\n",
      "Epoch 870 is train loss: 0.4165442634818839, train accuracy: 0.8333333333333334\n",
      "Epoch 875 is train loss: 0.4164351804869166, train accuracy: 0.8333333333333334\n",
      "Epoch 880 is train loss: 0.4163261360691156, train accuracy: 0.8333333333333334\n",
      "Epoch 885 is train loss: 0.41621712576796766, train accuracy: 0.8333333333333334\n",
      "Epoch 890 is train loss: 0.41610814525617296, train accuracy: 0.8333333333333334\n",
      "Epoch 895 is train loss: 0.4159991903340869, train accuracy: 0.8333333333333334\n",
      "Epoch 900 is train loss: 0.41589025692439663, train accuracy: 0.8333333333333334\n",
      "Epoch 905 is train loss: 0.4157813410670209, train accuracy: 0.8333333333333334\n",
      "Epoch 910 is train loss: 0.4156724389142233, train accuracy: 0.8333333333333334\n",
      "Epoch 915 is train loss: 0.4155635467259304, train accuracy: 0.8333333333333334\n",
      "Epoch 920 is train loss: 0.41545466086524285, train accuracy: 0.8333333333333334\n",
      "Epoch 925 is train loss: 0.41534577779413423, train accuracy: 0.8333333333333334\n",
      "Epoch 930 is train loss: 0.4152368940693255, train accuracy: 0.8333333333333334\n",
      "Epoch 935 is train loss: 0.4151280063383308, train accuracy: 0.8333333333333334\n",
      "Epoch 940 is train loss: 0.4150191113356632, train accuracy: 0.8333333333333334\n",
      "Epoch 945 is train loss: 0.4149102058791974, train accuracy: 0.8333333333333334\n",
      "Epoch 950 is train loss: 0.41480128686667794, train accuracy: 0.8333333333333334\n",
      "Epoch 955 is train loss: 0.4146923512723707, train accuracy: 0.8333333333333334\n",
      "Epoch 960 is train loss: 0.4145833961438489, train accuracy: 0.8333333333333334\n",
      "Epoch 965 is train loss: 0.41447441859890755, train accuracy: 0.8333333333333334\n",
      "Epoch 970 is train loss: 0.4143654158226021, train accuracy: 0.8333333333333334\n",
      "Epoch 975 is train loss: 0.4142563850644052, train accuracy: 0.8333333333333334\n",
      "Epoch 980 is train loss: 0.4141473236354762, train accuracy: 0.8333333333333334\n",
      "Epoch 985 is train loss: 0.41403822890603886, train accuracy: 0.8333333333333334\n",
      "Epoch 990 is train loss: 0.4139290983028623, train accuracy: 0.8333333333333334\n",
      "Epoch 995 is train loss: 0.413819929306841, train accuracy: 0.8333333333333334\n",
      "Epoch 1000 is train loss: 0.41371071945066945, train accuracy: 0.8333333333333334\n",
      "Trainig is done:\n",
      "the training loss is 0.7434065602174826 and the training accuracy is 0.8333333333333334\n",
      "the testing loss is 0.35841657659159154 and the testing accuracy is 0.8809523809523809\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot of the confusion matrix, where the rows represent the real data and the columns the predicted data:\n",
      "      2     1    3\n",
      "2  19.0   0.0  0.0\n",
      "1   1.0  12.0  2.0\n",
      "3   0.0   2.0  6.0\n"
     ]
    }
   ],
   "source": [
    "myMlp3.train(data2, labels2, 1000, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
